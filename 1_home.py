import os
import streamlit as st
import openai

from core.pipeline_builder import build_query_pipeline
from core.index_builder.inquiry_index_builder import load_inquiry_index
from core.index_builder.act_index_builder import load_act_index
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import Settings

import llama_index.core

os.environ["OPENAI_API_KEY"] = st.secrets.openai_key

llama_index.core.set_global_handler("arize_phoenix")

embed_model = OpenAIEmbedding(
    model="text-embedding-3-small",
)
Settings.embed_model = embed_model

st.set_page_config(
    page_title="ICT Construction Chatbot",
    page_icon="ğŸ‘·",
    layout="centered",
    initial_sidebar_state="auto",
    menu_items=None,
)
openai.api_key = st.secrets.openai_key
st.title("ICT ê±´ì„¤ ì»¨ì„¤í„´íŠ¸, powered by Wordbricks ğŸ‘·ğŸ’¬")

if "messages" not in st.session_state.keys():  # Initialize the chat messages history
    st.session_state.messages = [
        {
            "role": "assistant",
            "content": "ê¶ê¸ˆí•œ ì‚¬í•­ì„ ë¬¼ì–´ë³´ì„¸ìš”. ICT ê±´ì„¤ì— ëŒ€í•œ ì „ë¬¸ê°€ë´‡ì´ ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤.",
        }
    ]


@st.cache_resource(show_spinner=False)
def load_indexes():
    with st.spinner(text="ë°ì´í„°ë¥¼ ë¡œë”©ì¤‘ ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
        inquiry_index = load_inquiry_index()
        act_index = load_act_index()
        return {"inquiry": inquiry_index, "act": act_index}


indexes = load_indexes()
qp = build_query_pipeline(indexes)

if "query_pipeline" not in st.session_state.keys():  # Initialize the chat engine
    st.session_state.query_pipeline = qp

if prompt := st.chat_input("ì§ˆë¬¸"):  # Prompt for user input and save to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})

for message in st.session_state.messages:  # Display the prior chat messages
    with st.chat_message(message["role"]):
        st.write(message["content"])

# If last message is not from assistant, generate a new response
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("ìƒê°ì¤‘..."):
            response = st.session_state.query_pipeline.run(query_str=prompt)
            st.write(str(response))
            message = {"role": "assistant", "content": str(response)}
            st.session_state.messages.append(message)  # Add response to message history
